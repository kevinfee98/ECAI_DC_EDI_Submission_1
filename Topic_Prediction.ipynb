{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class MultivariateLSTM(tf.keras.Model):\n",
    "    def __init__(self, input_dim, lag, hidden_dim, num_layers, output_dim, dropout_rate=0.2, name=\"MultivariateLSTM\"):\n",
    "        super(MultivariateLSTM, self).__init__(name=name)\n",
    "        self.input_dim = input_dim\n",
    "        self.lag = lag\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Build submodules\n",
    "        self.input_dropout = layers.Dropout(self.dropout_rate)\n",
    "\n",
    "        # Create stacked LSTM layers\n",
    "        self.lstm_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            # For all but last layer, return_sequences=True\n",
    "            return_sequences = (i < self.num_layers - 1)\n",
    "            lstm = layers.LSTM(self.input_dim, return_sequences=return_sequences,\n",
    "                               name=f\"LSTM_{i+1}\")\n",
    "            self.lstm_layers.append(lstm)\n",
    "\n",
    "        # Final dense projection\n",
    "        self.output_dense = layers.Dense(self.output_dim, name=\"Output\")\n",
    "        self.softplus = layers.Activation(\"softplus\", name=\"NonNegativeOutput\")\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: shape (batch, lag, input_dim)\n",
    "        returns: (batch, output_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_dropout(inputs, training=training)\n",
    "\n",
    "        # Pass through stacked LSTMs\n",
    "        for i, lstm in enumerate(self.lstm_layers):\n",
    "            x = lstm(x, training=training)\n",
    "            # x has shape (batch, hidden_dim) after the last LSTM\n",
    "        outputs = self.output_dense(x)  # (batch, N)\n",
    "        outputs = self.softplus(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sympy import ceiling\n",
    "from tensorflow.keras import layers, models, losses, optimizers\n",
    "import json\n",
    "\n",
    "from tensorflow.python.layers.core import dropout\n",
    "\n",
    "\n",
    "def readDataset(file_path):\n",
    "    dict = {}\n",
    "    with open(file_path) as f:\n",
    "        dict = json.load(f)\n",
    "    return dict\n",
    "\n",
    "def remove_all_zeros_years(dict):\n",
    "    updated_dict = {}\n",
    "    isZero = True\n",
    "    for key, value in dict.items():\n",
    "        for v in value:\n",
    "            if v != 0:\n",
    "                isZero = False\n",
    "        if isZero == False:\n",
    "            updated_dict[key] = value\n",
    "    return updated_dict\n",
    "\n",
    "def get_years_topics(dict):\n",
    "    years = []\n",
    "    topics = []\n",
    "    for key, value in dict.items():\n",
    "        years.append(key)\n",
    "        for v in value:\n",
    "            uri_split = v.split('//')\n",
    "            uri_split = uri_split[1].split('/')\n",
    "            topics.append(uri_split[len(uri_split)-1])\n",
    "    return np.unique(years), np.unique(topics)\n",
    "\n",
    "def get_year_topics_freq(dict):\n",
    "    topic_freq = {}\n",
    "    for year, value in dict.items():\n",
    "        freqs = []\n",
    "        for v in value:\n",
    "            freqs.append(dict[year][v])\n",
    "        topic_freq[year] = freqs\n",
    "    return topic_freq\n",
    "def get_year_topics(dict):\n",
    "    year_topics = {}\n",
    "    for year, value in dict.items():\n",
    "        topics = []\n",
    "        for v in value:\n",
    "            uri_split = v.split('//')\n",
    "            uri_split = uri_split[1].split('/')\n",
    "            topic = uri_split[len(uri_split) - 1]\n",
    "            topics.append(topic)\n",
    "        year_topics[year] = topics\n",
    "    return year_topics\n",
    "\n",
    "topics_dict = readDataset(\"Data/topic_year_counts.json\")\n",
    "topics_dict = remove_all_zeros_years(topics_dict)\n",
    "years, topics = get_years_topics(topics_dict)\n",
    "topics_freq = get_year_topics_freq(topics_dict)\n",
    "print(f\"years of proceedings: {years}\\n\"\n",
    "      f\"Number of years: {len(years)}\\n\")\n",
    "print(f\"topics of proceedings: {topics}\\n\"\n",
    "      f\"Number of topics: {len(topics)}\\n\")\n",
    "print(f\"topics frequencies: {topics_freq}\\n\")\n",
    "# ----------------------------\n",
    "# Data preparation\n",
    "# ----------------------------\n",
    "# Suppose you have:\n",
    "# years: 1D array of years, length = Y\n",
    "# freqs: 2D array of shape (Y, N) with nonnegative counts per topic per year\n",
    "# Example dummy data (replace with real data)\n",
    "Y = len(years)  # number of years\n",
    "N = len(topics)  # number of topics\n",
    "def dict_to_freqs_array(freqs_dict):\n",
    "    years_sorted = sorted((int(k) for k in freqs_dict.keys()))\n",
    "    freq_rows = [np.asarray(freqs_dict[str(year)], dtype=np.float32) for year in years_sorted]\n",
    "    return np.stack(freq_rows, axis=0), years_sorted\n",
    "\n",
    "freqs, years_order = dict_to_freqs_array(topics_freq)\n",
    "\n",
    "Y, N = freqs.shape\n",
    "np.random.seed(0)\n",
    "#years = np.arange(2000, 2000 + Y)\n",
    "#freqs = topics_freq #np.random.poisson(lam=5.0, size=(Y, N)).astype(np.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "lags = 10  # how many past years to use for prediction\n",
    "train_ratio = 0.8  # train/val split\n",
    "batch_size = 16\n",
    "hidden_dim = 128\n",
    "num_layers = 5\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "\n",
    "# Create sequences of shape (num_samples, lags, N) for inputs and (num_samples, N) for targets\n",
    "def create_sequences(freqs, lags):\n",
    "    X, y = [], []\n",
    "    for t in range(lags, len(freqs)):\n",
    "        X.append(freqs[t - lags:t, :])  # (lags, N)\n",
    "        y.append(freqs[t, :])          # (N,)\n",
    "    X = np.stack(X)  # (samples, lags, N)\n",
    "    y = np.stack(y)  # (samples, N)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_sequences(freqs, lags)\n",
    "num_samples = X.shape[0]\n",
    "input_dim = N\n",
    "output_dim = N\n",
    "\n",
    "# Train/val split\n",
    "split = int(train_ratio * num_samples)\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "print(f\"X_train: {X_train}\\ny_train: {y_train}\")\n",
    "# Build TensorFlow Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "# ----------------------------\n",
    "# Model: Multivariate LSTM Regressor\n",
    "# ----------------------------\n",
    "from MultivariateLSTM import MultivariateLSTM\n",
    "# Instantiate the model\n",
    "model = MultivariateLSTM(input_dim=N, lag=lags,\n",
    "                         hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                         output_dim=N, dropout_rate=0.0)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss=losses.MeanAbsoluteError(),\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6,\n",
    "                                              restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    #validation_data=val_ds,\n",
    "    epochs=num_epochs,\n",
    "    #callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation / Inference\n",
    "# ----------------------------\n",
    "# Example: forecast the next year after the last observed years\n",
    "counts_above_zero = 0\n",
    "if freqs.shape[0] >= lags:\n",
    "    last_seq = freqs[-lags:, :]  # (lags, N)\n",
    "    last_seq = last_seq[np.newaxis, ...]  # (1, lag, N)\n",
    "    pred_next = model.predict(last_seq)  # (1, N)\n",
    "    pred_next = pred_next[0]  # (N,)\n",
    "    preds_dict = {}\n",
    "    for i in range(len(topics)):\n",
    "        if pred_next[i] < 0.5:\n",
    "            pred_next[i] = 0\n",
    "        else:\n",
    "            pred_next[i] = ceiling(pred_next[i])\n",
    "        preds_dict[str(topics[i])] = float(pred_next[i])\n",
    "        if pred_next[i] > 0.0:\n",
    "            counts_above_zero += 1\n",
    "    print(f\"Predicted Topic Frequencies for ECAI 2025:\\n{preds_dict}\")\n",
    "    print(f\"Total Predicted Topics: {counts_above_zero}\")\n",
    "    pretty_json = json.dumps(preds_dict, indent=4)\n",
    "    print(pretty_json)\n",
    "    with open(\"Data/ECAI_2025_Topic_Predictions.json\", \"w\") as f:\n",
    "        json.dump(preds_dict, f, indent=4)\n",
    "else:\n",
    "    print(\"Not enough data to generate prediction with the given lag.\")"
   ]
  }
 ]
}
